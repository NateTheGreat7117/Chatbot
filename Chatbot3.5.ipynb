{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02389e8-5f09-4716-8b02-1b8409b6dc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM, TrainingArguments, \\\n",
    "                         Trainer, DataCollatorForSeq2Seq\n",
    "from transformers.keras_callbacks import KerasMetricCallback\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datasets\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3a9418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid out of memory errors\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5ac7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d05b1d",
   "metadata": {},
   "source": [
    "# Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51515072",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkpoint = \"google/flan-t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d5073e-9fec-4d6d-88d5-daab9c80b6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = []\n",
    "response = []\n",
    "\n",
    "with open(\"D:/Users/Natha/Datasets/MyJarvisConversation/conversation.txt\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        if line[0] == \"U\":\n",
    "            query.append(line[6:].split(\"\\n\")[0])\n",
    "        elif line[0] == \"J\":\n",
    "            response.append(line[8:].split(\"\\n\")[0])\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccfe2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data =  {\"query\": query,\n",
    "         \"response\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2da356",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.Dataset.from_pandas(pd.DataFrame(data=data))\n",
    "dataset = datasets.Dataset.from_pandas(pd.DataFrame(data={\"conversation\": dataset}))\n",
    "dataset = dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22194cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_lang = \"query\"\n",
    "target_lang = \"response\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd42e956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = [example[source_lang] for example in examples[\"conversation\"]]\n",
    "    targets = [example[target_lang] for example in examples[\"conversation\"]]\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True)\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f54260d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_ds = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff8b5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8770168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5105d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9fd1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(checkpoint, from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30dd93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_set = model.prepare_tf_dataset(\n",
    "    tokenized_ds[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_val_set = model.prepare_tf_dataset(\n",
    "    tokenized_ds[\"test\"],\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e38a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58aae3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b73cc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=tf_train_set, epochs=30, validation_data=tf_val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ca820b",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    input_text = input(\"Enter text: \")\n",
    "    tokenized  = tokenizer(input_text, return_tensors=\"tf\").input_ids\n",
    "    prediction = model.generate(tokenized, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)\n",
    "    output_text = tokenizer.decode(prediction[0], skip_special_tokens=True)\n",
    "    \n",
    "    response = output_text.lower()\n",
    "        # Get the date\n",
    "    if \"/udate\" in response:\n",
    "        response = response.replace(\"/udate\", datetime.date.today().strftime(\"%B %d, %Y\"))\n",
    "    # Get the time\n",
    "    if \"/utime\" in response:\n",
    "        response = response.replace(\"/utime\", datetime.datetime.now().strftime(\"%I:%M:%S\"))\n",
    "    # Get the temperature\n",
    "    if \"/utemp\" in response:\n",
    "        response = response.replace(\"/utemp\", weather(\"boston\", \"tm\"))\n",
    "    # Get the humidity\n",
    "    if \"/uhumidity\" in response:\n",
    "        response = response.replace(\"/uhumidity\", weather(\"boston\", \"hm\"))\n",
    "    # Get the wind speed\n",
    "    if \"/uwind\" in response:\n",
    "        response = response.replace(\"/uwind\", weather(\"boston\", \"ws\"))\n",
    "    # Get the amount of precipitation\n",
    "    if \"/uprecipitation\" in response:\n",
    "        response = response.replace(\"/uprecipitation\", weather(\"boston\", \"pp\"))\n",
    "    if \"/uvolume\" in response:\n",
    "        after = response.split(\"/uvolume\")[-1]\n",
    "        vol = after.split(\"'\")[1]\n",
    "        #response = response.replace(\"/uvolume\"+\"'\"+vol+\"'\", \"\")\n",
    "        volume(vol)\n",
    "    if \"/usleep\" in response:\n",
    "        response = response.replace(\"/usleep\", \"\")\n",
    "        print(response)\n",
    "        break\n",
    "    if \"/unewtab\" in response:\n",
    "        response = response.replace(\"/unewtab\", \"\")\n",
    "        pyautogui.hotkey('ctrl', 't')\n",
    "    if \"/uclosetab\" in response:\n",
    "        response = response.replace(\"/uclosetab\", \"\")\n",
    "        pyautogui.hotkey('ctrl', 'w')\n",
    "    if \"/uswitchtab\" in response:\n",
    "        after = response.split(\"/uswitchtab\")[-1]\n",
    "        new = after.split(\"'\")[1]\n",
    "        response = response.replace(\"/uswitchtab\"+\"'\"+new+\"'\", \"\")\n",
    "        pyautogui.hotkey('ctrl', str(word2num(new)))\n",
    "    \n",
    "    print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6155b98f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
