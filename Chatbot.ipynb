{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a02389e8-5f09-4716-8b02-1b8409b6dc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForCausalLM, AutoModelForCausalLM, \\\n",
    "                         TFAutoModelForQuestionAnswering, TrainingArguments, Trainer, \\\n",
    "                         DataCollatorForLanguageModeling\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datasets\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc3a9418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid out of memory errors\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c5ac7e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600a2710-d528-418c-8746-08b75fb23940",
   "metadata": {},
   "source": [
    "# Get Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51515072",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "model = TFAutoModelForCausalLM.from_pretrained(\"distilgpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72036e07-bf66-47bd-ae96-15adfcbbfd68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jarvis's Persona: An AI assistant that is calm, sophisticated, and dependable with a touch of dry wit\n",
      "<START>\n",
      "You: Hey Jarvis you ready?\n",
      "[CHARACTER]: Thanks for taking this opportunity to test the Persona experience. I've started thinking about doing certain things that I don't want to do, or that need to change.\n"
     ]
    }
   ],
   "source": [
    "input_text = '''Jarvis's Persona: An AI assistant that is calm, sophisticated, and dependable with a touch of dry wit\n",
    "<START>\n",
    "You: Hey Jarvis you ready?\n",
    "[CHARACTER]:'''\n",
    "\n",
    "tokens = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "output = model.generate(tokens, max_length=75, temperature=0.8, do_sample=True)\n",
    "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3829ffe8-b991-45bd-b406-b0986fbed1cf",
   "metadata": {},
   "source": [
    "# Collect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9d5073e-9fec-4d6d-88d5-daab9c80b6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = []\n",
    "response = []\n",
    "\n",
    "with open(\"D:/Users/Natha/Datasets/MyJarvisConversation/conversation.txt\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        if line[0] == \"U\":\n",
    "            query.append(line[6:].split(\"\\n\")[0])\n",
    "        elif line[0] == \"J\":\n",
    "            response.append(line[8:].split(\"\\n\")[0])\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b36dda18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "395"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_tokens(query + response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96ff042d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"query\": query,\n",
    "        \"response\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5a54726",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.Dataset.from_pandas(pd.DataFrame(data=data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd6b9de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerWrapper:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def tokenize_function(self, examples):\n",
    "        return self.tokenizer([\" \".join(x) for x in examples[\"response\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82f2bdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_wrapper = TokenizerWrapper(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c0e86bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/241 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_ds = dataset.map(tokenizer_wrapper.tokenize_function,\n",
    "                           batched=True,\n",
    "                           num_proc=4,\n",
    "                           remove_columns=dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77b9b279",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b68c982",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessWrapper():\n",
    "    def __init__(self, block_size):\n",
    "        self.block_size = block_size\n",
    "        \n",
    "    def group_texts(self, examples):\n",
    "        # Concatenate all texts.\n",
    "        concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "        total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "        # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "        if total_length >= self.block_size:\n",
    "            total_length = (total_length // self.block_size) * self.block_size\n",
    "        # Split by chunks of block_size.\n",
    "        result = {\n",
    "            k: [t[i : i + self.block_size] for i in range(0, total_length, self.block_size)]\n",
    "            for k, t in concatenated_examples.items()\n",
    "        }\n",
    "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "655b0506",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_wrapper = PreprocessWrapper(block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "225c3cce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[32,\n",
       " 50414,\n",
       " 82,\n",
       " 50414,\n",
       " 50414,\n",
       " 50414,\n",
       " 64,\n",
       " 50414,\n",
       " 75,\n",
       " 50414,\n",
       " 86,\n",
       " 50414,\n",
       " 64,\n",
       " 50414,\n",
       " 88,\n",
       " 50414,\n",
       " 82,\n",
       " 50414,\n",
       " 11,\n",
       " 50414,\n",
       " 50414,\n",
       " 50414,\n",
       " 64,\n",
       " 50414,\n",
       " 50414,\n",
       " 50414,\n",
       " 79,\n",
       " 50414,\n",
       " 75,\n",
       " 50414,\n",
       " 68,\n",
       " 50414,\n",
       " 64,\n",
       " 50414,\n",
       " 82,\n",
       " 50414,\n",
       " 84,\n",
       " 50414,\n",
       " 81,\n",
       " 50414,\n",
       " 68,\n",
       " 50414,\n",
       " 50414,\n",
       " 50414,\n",
       " 83,\n",
       " 50414,\n",
       " 78,\n",
       " 50414,\n",
       " 50414,\n",
       " 50414,\n",
       " 82,\n",
       " 50414,\n",
       " 68,\n",
       " 50414,\n",
       " 68,\n",
       " 50414,\n",
       " 50414,\n",
       " 50414,\n",
       " 88,\n",
       " 50414,\n",
       " 78,\n",
       " 50414,\n",
       " 84,\n",
       " 50414,\n",
       " 50414,\n",
       " 50414,\n",
       " 82,\n",
       " 50414,\n",
       " 72,\n",
       " 50414,\n",
       " 81]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds[0][\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b6689da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/241 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm_dataset = tokenized_ds.map(preprocess_wrapper.group_texts, batched=True, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90c41cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d05a1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "tf_train_set = model.prepare_tf_dataset(\n",
    "    lm_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac788c1-bcc4-426b-8a8d-bbaac115238a",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd677b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import create_optimizer, AdamWeightDecay\n",
    "\n",
    "optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d5a58be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b07ee26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'tfgpt2lm_head_model/transformer/assert_less/Assert/Assert' defined at (most recent call last):\n    File \"C:\\Users\\Natha\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\Natha\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\Natha\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\Natha\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"C:\\Users\\Natha\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"C:\\Users\\Natha\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\Natha\\anaconda3\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"C:\\Users\\Natha\\anaconda3\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"C:\\Users\\Natha\\anaconda3\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\Natha\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\Natha\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 460, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\Natha\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 367, in dispatch_shell\n      await result\n    File \"C:\\Users\\Natha\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 662, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\Natha\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 360, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"C:\\Users\\Natha\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\Natha\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2863, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\Natha\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2909, in _run_cell\n      return runner(coro)\n    File \"C:\\Users\\Natha\\anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\Natha\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3106, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\Natha\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3309, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\Natha\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3369, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Natha\\AppData\\Local\\Temp\\ipykernel_1096\\1326729149.py\", line 1, in <cell line: 1>\n      model.fit(tf_train_set, epochs=5)\n    File \"C:\\Users\\Natha\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Natha\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1409, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\Natha\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\Natha\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Natha\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\Natha\\anaconda3\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1658, in train_step\n      y_pred = self(x, training=True)\n    File \"C:\\Users\\Natha\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Natha\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 490, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"C:\\Users\\Natha\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Natha\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\Natha\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Natha\\anaconda3\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 805, in run_call_with_unpacked_inputs\n      try:\n    File \"C:\\Users\\Natha\\anaconda3\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_tf_gpt2.py\", line 837, in call\n      transformer_outputs = self.transformer(\n    File \"C:\\Users\\Natha\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Natha\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\Natha\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Natha\\anaconda3\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 805, in run_call_with_unpacked_inputs\n      try:\n    File \"C:\\Users\\Natha\\anaconda3\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_tf_gpt2.py\", line 437, in call\n      if inputs_embeds is None:\n    File \"C:\\Users\\Natha\\anaconda3\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_tf_gpt2.py\", line 438, in call\n      check_embeddings_within_bounds(input_ids, self.config.vocab_size)\n    File \"C:\\Users\\Natha\\anaconda3\\lib\\site-packages\\transformers\\tf_utils.py\", line 161, in check_embeddings_within_bounds\n      tf.debugging.assert_less(\nNode: 'tfgpt2lm_head_model/transformer/assert_less/Assert/Assert'\nassertion failed: [The maximum value of input_ids (Tensor(\\\"tfgpt2lm_head_model/transformer/Max:0\\\", shape=(), dtype=int32)) must be smaller than the embedding layer\\'s input dimension (50257). The likely cause is some problem at tokenization time.] [Condition x < y did not hold element-wise:] [x (tfgpt2lm_head_model/transformer/Reshape:0) = ] [[64 50414 84...]...] [y (tfgpt2lm_head_model/transformer/Cast_1/x:0) = ] [50257]\n\t [[{{node tfgpt2lm_head_model/transformer/assert_less/Assert/Assert}}]] [Op:__inference_train_function_58687]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[1;32mIn [37]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf_train_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'tfgpt2lm_head_model/transformer/assert_less/Assert/Assert' defined at (most recent call last):\n    File \"C:\\Users\\Natha\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\Natha\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\Natha\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\Natha\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"C:\\Users\\Natha\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"C:\\Users\\Natha\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\Natha\\anaconda3\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"C:\\Users\\Natha\\anaconda3\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"C:\\Users\\Natha\\anaconda3\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\Natha\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\Natha\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 460, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\Natha\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 367, in dispatch_shell\n      await result\n    File \"C:\\Users\\Natha\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 662, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\Natha\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 360, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"C:\\Users\\Natha\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\Natha\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2863, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\Natha\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2909, in _run_cell\n      return runner(coro)\n    File \"C:\\Users\\Natha\\anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\Natha\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3106, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\Natha\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3309, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\Natha\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3369, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Natha\\AppData\\Local\\Temp\\ipykernel_1096\\1326729149.py\", line 1, in <cell line: 1>\n      model.fit(tf_train_set, epochs=5)\n    File \"C:\\Users\\Natha\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Natha\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1409, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\Natha\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\Natha\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Natha\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\Natha\\anaconda3\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1658, in train_step\n      y_pred = self(x, training=True)\n    File \"C:\\Users\\Natha\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Natha\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 490, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"C:\\Users\\Natha\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Natha\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\Natha\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Natha\\anaconda3\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 805, in run_call_with_unpacked_inputs\n      try:\n    File \"C:\\Users\\Natha\\anaconda3\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_tf_gpt2.py\", line 837, in call\n      transformer_outputs = self.transformer(\n    File \"C:\\Users\\Natha\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Natha\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\Natha\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Natha\\anaconda3\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 805, in run_call_with_unpacked_inputs\n      try:\n    File \"C:\\Users\\Natha\\anaconda3\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_tf_gpt2.py\", line 437, in call\n      if inputs_embeds is None:\n    File \"C:\\Users\\Natha\\anaconda3\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_tf_gpt2.py\", line 438, in call\n      check_embeddings_within_bounds(input_ids, self.config.vocab_size)\n    File \"C:\\Users\\Natha\\anaconda3\\lib\\site-packages\\transformers\\tf_utils.py\", line 161, in check_embeddings_within_bounds\n      tf.debugging.assert_less(\nNode: 'tfgpt2lm_head_model/transformer/assert_less/Assert/Assert'\nassertion failed: [The maximum value of input_ids (Tensor(\\\"tfgpt2lm_head_model/transformer/Max:0\\\", shape=(), dtype=int32)) must be smaller than the embedding layer\\'s input dimension (50257). The likely cause is some problem at tokenization time.] [Condition x < y did not hold element-wise:] [x (tfgpt2lm_head_model/transformer/Reshape:0) = ] [[64 50414 84...]...] [y (tfgpt2lm_head_model/transformer/Cast_1/x:0) = ] [50257]\n\t [[{{node tfgpt2lm_head_model/transformer/assert_less/Assert/Assert}}]] [Op:__inference_train_function_58687]"
     ]
    }
   ],
   "source": [
    "model.fit(tf_train_set, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f03184",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
